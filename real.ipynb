{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02cd56d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to UR3e successfully!\n",
      "Current joint positions: [0.001344919204711914, -1.7922941646971644, 0.5159648100482386, -1.433665008550026, -0.03783637682069951, -0.00012380281557256012]\n"
     ]
    }
   ],
   "source": [
    "import rtde_control\n",
    "from rtde_control import RTDEControlInterface as RTDEControl\n",
    "import rtde_receive\n",
    "import time\n",
    "\n",
    "# Robot IP address\n",
    "ROBOT_IP = \"147.175.108.138\"\n",
    "\n",
    "try:\n",
    "    # Initialize RTDE connections\n",
    "    rtde_c = rtde_control.RTDEControlInterface(ROBOT_IP, RTDEControl.FLAG_USE_EXT_UR_CAP)\n",
    "    rtde_r = rtde_receive.RTDEReceiveInterface(ROBOT_IP)\n",
    "    \n",
    "    print(\"Connected to UR3e successfully!\")\n",
    "    \n",
    "    # Example: Get current joint positions\n",
    "    joint_positions = rtde_r.getActualQ()\n",
    "    print(f\"Current joint positions: {joint_positions}\")\n",
    "    \n",
    "    # Example: Move robot (be careful!)\n",
    "    rtde_c.moveJ([-0.7866123358355921, -1.562515858826675, -0.002566894283518195, -1.5586068074083705, 0.00971465464681387, 0.014539957046508789], 0.5, 0.5)\n",
    "    #rtde_c.moveJ([0.0, -1.712, 1.712, 0.0, 0.0, 0.0], 0.2, 0.2)\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "finally:\n",
    "    if 'rtde_c' in locals():\n",
    "        rtde_c.disconnect()\n",
    "    if 'rtde_r' in locals():\n",
    "        rtde_r.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import rtde_control\n",
    "import rtde_receive\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from rtde_control import RTDEControlInterface as RTDEControl\n",
    "\n",
    "\n",
    "\n",
    "class UR3eReach(gym.Env):\n",
    "    def __init__(self, robot_ip=\"147.175.108.138\"):\n",
    "        self.rtde_c = rtde_control.RTDEControlInterface(robot_ip, RTDEControl.FLAG_USE_EXT_UR_CAP)\n",
    "        self.rtde_r = rtde_receive.RTDEReceiveInterface(robot_ip)\n",
    "\n",
    "        # Observations:  Box(-inf, inf, (25,), float32)\n",
    "        # Actions:  Box(-1.0, 1.0, (6,), float32)\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=-float(\"inf\"), high=float(\"inf\"), shape=(25,), dtype=float)\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(6,), dtype=float)\n",
    "\n",
    "        self.default_joint_position = [0.0, -1.712, 1.712, 0.0, 0.0, 0.0]\n",
    "\n",
    "        self.last_actions = torch.zeros((6, 1))\n",
    "\n",
    "    def _get_observations(self):\n",
    "        try:\n",
    "            joint_positions = self.rtde_r.getActualQ() # 6\n",
    "            joint_velocities = self.rtde_r.getActualQd() # 6\n",
    "\n",
    "            tool_center = self.rtde_r.getActualTCPPose()\n",
    "\n",
    "            goal_position = self._generate_goal_position() # 3\n",
    "            goal_rotation = self._generate_goal_rotation() # 4 - should be quaternion\n",
    "\n",
    "            #end_effector_pose = self.rtde_r.get_tool_pose()\n",
    "            print(\"Joint Positions:\", joint_positions)\n",
    "            #print(\"End Effector Pose:\", end_effector_pose)\n",
    "\n",
    "            return {\n",
    "                \"joint_positions\": joint_positions,\n",
    "                \"joint_velocities\": joint_velocities,\n",
    "                \"tool_center\": tool_center,\n",
    "                \"goal_position\": goal_position,\n",
    "                \"goal_rotation\": goal_rotation,\n",
    "                \"last_actions\": self.last_actions\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error getting observations:\", e)\n",
    "            return self.default_joint_position + [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "    def _generate_goal_position(self):\n",
    "        # self.commands.ee_pose.ranges.pos_x = (0.35, 0.4)\n",
    "        # self.commands.ee_pose.ranges.pos_y = (-0.1, 0.1)\n",
    "        # self.commands.ee_pose.ranges.pos_z = (0.15, 0.45)\n",
    "\n",
    "        range_x = (0.35, 0.4)\n",
    "        range_y = (-0.1, 0.1)\n",
    "        range_z = (0.15, 0.45)\n",
    "\n",
    "        goal = [\n",
    "            np.random.uniform(*range_x),\n",
    "            np.random.uniform(*range_y),\n",
    "            np.random.uniform(*range_z),\n",
    "        ]\n",
    "\n",
    "        return list(goal)\n",
    "    \n",
    "    def _generate_goal_rotation(self):\n",
    "        # self.commands.ee_pose.ranges.pitch = (math.pi / 2, math.pi / 2)\n",
    "\n",
    "        goal = [\n",
    "            0.0,\n",
    "            np.random.uniform(np.pi / 2, np.pi / 2),\n",
    "            np.random.uniform(-np.pi, np.pi)\n",
    "        ]\n",
    "\n",
    "        return list(goal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c25036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((6, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91f4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UR3eReach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530acf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint Positions: [-0.7929261366473597, -1.5604003363153716, 0.00605947176088506, -1.5565631550601502, 0.00436015147715807, 0.016711734235286713]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.7929261366473597,\n",
       "  -1.5604003363153716,\n",
       "  0.00605947176088506,\n",
       "  -1.5565631550601502,\n",
       "  0.00436015147715807,\n",
       "  0.016711734235286713,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.27393328901869574,\n",
       "  -0.25616682315174233,\n",
       "  0.6937997953184033,\n",
       "  -0.7835107966469409,\n",
       "  -1.7507451077993939,\n",
       "  1.7847540814308716,\n",
       "  0.38270703473242024,\n",
       "  -0.021508604535532133,\n",
       "  0.29455592611941017,\n",
       "  0.0,\n",
       "  1.5707963267948966,\n",
       "  2.2644955995101927],\n",
       " 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env._get_observations()\n",
    "obs, len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdaf738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39202029101843633, 0.038021564112655476, 0.22879126602887667]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._generate_goal_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20f6414",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'frankx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfrankx\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReachingFranka\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, robot_ip\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m172.16.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m, control_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, motion_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaypoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, camera_tracking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# gym API\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'frankx'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "\n",
    "import frankx\n",
    "\n",
    "\n",
    "class ReachingFranka(gym.Env):\n",
    "    def __init__(self, robot_ip=\"172.16.0.2\", device=\"cuda:0\", control_space=\"joint\", motion_type=\"waypoint\", camera_tracking=False):\n",
    "        # gym API\n",
    "        self._drepecated_api = version.parse(gym.__version__) < version.parse(\" 0.25.0\")\n",
    "\n",
    "        self.device = device\n",
    "        self.control_space = control_space  # joint or cartesian\n",
    "        self.motion_type = motion_type  # waypoint or impedance\n",
    "\n",
    "        if self.control_space == \"cartesian\" and self.motion_type == \"impedance\":\n",
    "            # The operation of this mode (Cartesian-impedance) was adjusted later without being able to test it on the real robot.\n",
    "            # Dangerous movements may occur for the operator and the robot.\n",
    "            # Comment the following line of code if you want to proceed with this mode.\n",
    "            raise ValueError(\"See comment in the code to proceed with this mode\")\n",
    "            pass\n",
    "\n",
    "        # camera tracking (disabled by default)\n",
    "        self.camera_tracking = camera_tracking\n",
    "        if self.camera_tracking:\n",
    "            threading.Thread(target=self._update_target_from_camera).start()\n",
    "\n",
    "        # spaces\n",
    "        self.observation_space = gym.spaces.Box(low=-1000, high=1000, shape=(18,), dtype=np.float32)\n",
    "        if self.control_space == \"joint\":\n",
    "            self.action_space = gym.spaces.Box(low=-1, high=1, shape=(7,), dtype=np.float32)\n",
    "        elif self.control_space == \"cartesian\":\n",
    "            self.action_space = gym.spaces.Box(low=-1, high=1, shape=(3,), dtype=np.float32)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid control space:\", self.control_space)\n",
    "\n",
    "        # init real franka\n",
    "        print(\"Connecting to robot at {}...\".format(robot_ip))\n",
    "        self.robot = frankx.Robot(robot_ip)\n",
    "        self.robot.set_default_behavior()\n",
    "        self.robot.recover_from_errors()\n",
    "\n",
    "        # the robot's response can be better managed by independently setting the following properties, for example:\n",
    "        # - self.robot.velocity_rel = 0.2\n",
    "        # - self.robot.acceleration_rel = 0.1\n",
    "        # - self.robot.jerk_rel = 0.01\n",
    "        self.robot.set_dynamic_rel(0.25)\n",
    "\n",
    "        self.gripper = self.robot.get_gripper()\n",
    "        print(\"Robot connected\")\n",
    "\n",
    "        self.motion = None\n",
    "        self.motion_thread = None\n",
    "\n",
    "        self.dt = 1 / 120.0\n",
    "        self.action_scale = 2.5\n",
    "        self.dof_vel_scale = 0.1\n",
    "        self.max_episode_length = 100\n",
    "        self.robot_dof_speed_scales = 1\n",
    "        self.target_pos = np.array([0.65, 0.2, 0.2])\n",
    "        self.robot_default_dof_pos = np.radians([0, -45, 0, -135, 0, 90, 45])\n",
    "        self.robot_dof_lower_limits = np.array([-2.8973, -1.7628, -2.8973, -3.0718, -2.8973, -0.0175, -2.8973])\n",
    "        self.robot_dof_upper_limits = np.array([ 2.8973,  1.7628,  2.8973, -0.0698,  2.8973,  3.7525,  2.8973])\n",
    "\n",
    "        self.progress_buf = 1\n",
    "        self.obs_buf = np.zeros((18,), dtype=np.float32)\n",
    "\n",
    "    def _update_target_from_camera(self):\n",
    "        pixel_to_meter = 1.11 / 375  # m/px: adjust for custom cases\n",
    "\n",
    "        import cv2\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # convert to HSV and remove noise\n",
    "            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            hsv = cv2.medianBlur(hsv, 15)\n",
    "\n",
    "            # color matching in HSV\n",
    "            mask = cv2.inRange(hsv, np.array([80, 100, 100]), np.array([100, 255, 255]))\n",
    "            M = cv2.moments(mask)\n",
    "            if M[\"m00\"]:\n",
    "                x = M[\"m10\"] / M[\"m00\"]\n",
    "                y = M[\"m01\"] / M[\"m00\"]\n",
    "\n",
    "                # real-world position (fixed z to 0.2 meters)\n",
    "                pos = np.array([pixel_to_meter * (y - 185), pixel_to_meter * (x - 320), 0.2])\n",
    "                if self is not None:\n",
    "                    self.target_pos = pos\n",
    "\n",
    "                # draw target\n",
    "                frame = cv2.circle(frame, (int(x), int(y)), 30, (0,0,255), 2)\n",
    "                frame = cv2.putText(frame, str(np.round(pos, 4).tolist()), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # show images\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "            cv2.imshow(\"mask\", mask)\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k == ord('q'):\n",
    "                cap.release()\n",
    "\n",
    "    def _get_observation_reward_done(self):\n",
    "        # get robot state\n",
    "        try:\n",
    "            robot_state = self.robot.get_state(read_once=True)\n",
    "        except frankx.InvalidOperationException:\n",
    "            robot_state = self.robot.get_state(read_once=False)\n",
    "\n",
    "        # observation\n",
    "        robot_dof_pos = np.array(robot_state.q)\n",
    "        robot_dof_vel = np.array(robot_state.dq)\n",
    "        end_effector_pos = np.array(robot_state.O_T_EE[-4:-1])\n",
    "\n",
    "        dof_pos_scaled = 2.0 * (robot_dof_pos - self.robot_dof_lower_limits) / (self.robot_dof_upper_limits - self.robot_dof_lower_limits) - 1.0\n",
    "        dof_vel_scaled = robot_dof_vel * self.dof_vel_scale\n",
    "\n",
    "        self.obs_buf[0] = self.progress_buf / float(self.max_episode_length)\n",
    "        self.obs_buf[1:8] = dof_pos_scaled\n",
    "        self.obs_buf[8:15] = dof_vel_scaled\n",
    "        self.obs_buf[15:18] = self.target_pos\n",
    "\n",
    "        # reward\n",
    "        distance = np.linalg.norm(end_effector_pos - self.target_pos)\n",
    "        reward = -distance\n",
    "\n",
    "        # done\n",
    "        done = self.progress_buf >= self.max_episode_length - 1\n",
    "        done = done or distance <= 0.075\n",
    "\n",
    "        print(\"Distance:\", distance)\n",
    "        if done:\n",
    "            print(\"Target or Maximum episode length reached\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        return self.obs_buf, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"Resetting...\")\n",
    "\n",
    "        # end current motion\n",
    "        if self.motion is not None:\n",
    "            self.motion.finish()\n",
    "            self.motion_thread.join()\n",
    "        self.motion = None\n",
    "        self.motion_thread = None\n",
    "\n",
    "        # open/close gripper\n",
    "        # self.gripper.open()\n",
    "        # self.gripper.clamp()\n",
    "\n",
    "        # go to 1) safe position, 2) random position\n",
    "        self.robot.move(frankx.JointMotion(self.robot_default_dof_pos.tolist()))\n",
    "        dof_pos = self.robot_default_dof_pos + 0.25 * (np.random.rand(7) - 0.5)\n",
    "        self.robot.move(frankx.JointMotion(dof_pos.tolist()))\n",
    "\n",
    "        # get target position from prompt\n",
    "        if not self.camera_tracking:\n",
    "            while True:\n",
    "                try:\n",
    "                    print(\"Enter target position (X, Y, Z) in meters\")\n",
    "                    raw = input(\"or press [Enter] key for a random target position: \")\n",
    "                    if raw:\n",
    "                        self.target_pos = np.array([float(p) for p in raw.replace(' ', '').split(',')])\n",
    "                    else:\n",
    "                        noise = (2 * np.random.rand(3) - 1) * np.array([0.25, 0.25, 0.10])\n",
    "                        self.target_pos = np.array([0.5, 0.0, 0.2]) + noise\n",
    "                    print(\"Target position:\", self.target_pos)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Try something like: 0.65, 0.0, 0.2\")\n",
    "\n",
    "        # initial pose\n",
    "        affine = frankx.Affine(frankx.Kinematics.forward(dof_pos.tolist()))\n",
    "        affine = affine * frankx.Affine(x=0, y=0, z=-0.10335, a=np.pi/2)\n",
    "\n",
    "        # motion type\n",
    "        if self.motion_type == \"waypoint\":\n",
    "            self.motion = frankx.WaypointMotion([frankx.Waypoint(affine)], return_when_finished=False)\n",
    "        elif self.motion_type == \"impedance\":\n",
    "            self.motion = frankx.ImpedanceMotion(500, 50)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid motion type:\", self.motion_type)\n",
    "\n",
    "        self.motion_thread = self.robot.move_async(self.motion)\n",
    "        if self.motion_type == \"impedance\":\n",
    "            self.motion.target = affine\n",
    "\n",
    "        input(\"Press [Enter] to continue\")\n",
    "\n",
    "        self.progress_buf = 0\n",
    "        observation, reward, done = self._get_observation_reward_done()\n",
    "\n",
    "        if self._drepecated_api:\n",
    "            return observation\n",
    "        else:\n",
    "            return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.progress_buf += 1\n",
    "\n",
    "        # control space\n",
    "        # joint\n",
    "        if self.control_space == \"joint\":\n",
    "            # get robot state\n",
    "            try:\n",
    "                robot_state = self.robot.get_state(read_once=True)\n",
    "            except frankx.InvalidOperationException:\n",
    "                robot_state = self.robot.get_state(read_once=False)\n",
    "            # forward kinematics\n",
    "            dof_pos = np.array(robot_state.q) + (self.robot_dof_speed_scales * self.dt * action * self.action_scale)\n",
    "            affine = frankx.Affine(self.robot.forward_kinematics(dof_pos.flatten().tolist()))\n",
    "            affine = affine * frankx.Affine(x=0, y=0, z=-0.10335, a=np.pi/2)\n",
    "        # cartesian\n",
    "        elif self.control_space == \"cartesian\":\n",
    "            action /= 100.0\n",
    "            if self.motion_type == \"waypoint\":\n",
    "                affine = frankx.Affine(x=action[0], y=action[1], z=action[2])\n",
    "            elif self.motion_type == \"impedance\":\n",
    "                # get robot pose\n",
    "                try:\n",
    "                    robot_pose = self.robot.current_pose(read_once=True)\n",
    "                except frankx.InvalidOperationException:\n",
    "                    robot_pose = self.robot.current_pose(read_once=False)\n",
    "                affine = robot_pose * frankx.Affine(x=action[0], y=action[1], z=action[2])\n",
    "\n",
    "        # motion type\n",
    "        # waypoint motion\n",
    "        if self.motion_type == \"waypoint\":\n",
    "            if self.control_space == \"joint\":\n",
    "                self.motion.set_next_waypoint(frankx.Waypoint(affine))\n",
    "            elif self.control_space == \"cartesian\":\n",
    "                self.motion.set_next_waypoint(frankx.Waypoint(affine, frankx.Waypoint.Relative))\n",
    "        # impedance motion\n",
    "        elif self.motion_type == \"impedance\":\n",
    "            self.motion.target = affine\n",
    "        else:\n",
    "            raise ValueError(\"Invalid motion type:\", self.motion_type)\n",
    "\n",
    "        # the use of time.sleep is for simplicity. This does not guarantee control at a specific frequency\n",
    "        time.sleep(0.1)  # lower frequency, at 30Hz there are discontinuities\n",
    "\n",
    "        observation, reward, done = self._get_observation_reward_done()\n",
    "\n",
    "        if self._drepecated_api:\n",
    "            return observation, reward, done, {}\n",
    "        else:\n",
    "            return observation, reward, done, done, {}\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1aefd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UR3e Model Testing Suite\n",
      "========================\n",
      "\n",
      "============================================================\n",
      "STEP 1: Loading and Inspecting Model\n",
      "============================================================\n",
      "Using device: cpu\n",
      "\n",
      "Loading model from: /home/urkui-3/Documents/Ales/IsaacLab/runs/torch/Isaac-Reach-UR3e-v0/UR3e/checkpoints/best_agent.pt\n",
      "\n",
      "--- Checkpoint Structure ---\n",
      "Checkpoint type: <class 'dict'>\n",
      "Checkpoint keys: ['policy', 'value', 'optimizer', 'state_preprocessor', 'value_preprocessor']\n",
      "❌ Unexpected checkpoint structure\n",
      "\n",
      "❌ Failed to load model. Please check the checkpoint structure.\n",
      "\n",
      "============================================================\n",
      "STEP 2: Creating Mock Observations\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 3: Testing Model Inference\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urkui-3/miniconda3/envs/env_isaaclab/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/tmp/ipykernel_429053/2345314918.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 488\u001b[0m\n\u001b[1;32m    485\u001b[0m observations, descriptions \u001b[38;5;241m=\u001b[39m create_mock_observations()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Step 3: Test inference\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Step 4: Analyze actions\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 295\u001b[0m, in \u001b[0;36mtest_model_inference\u001b[0;34m(model, device, observations, descriptions)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTEP 3: Testing Model Inference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m    296\u001b[0m actions_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "def test_model_loading(model_path: str):\n",
    "    \"\"\"Test loading the model and inspect its structure\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: Loading and Inspecting Model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    print(f\"\\nLoading model from: {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Inspect checkpoint structure\n",
    "    print(\"\\n--- Checkpoint Structure ---\")\n",
    "    print(f\"Checkpoint type: {type(checkpoint)}\")\n",
    "    \n",
    "    # If checkpoint is OrderedDict, it's likely a state_dict\n",
    "    if isinstance(checkpoint, dict):\n",
    "        print(f\"Checkpoint keys: {list(checkpoint.keys())[:10]}\")  # Show first 10 keys\n",
    "        \n",
    "        # Check if it's a state dict by looking at the keys\n",
    "        sample_keys = list(checkpoint.keys())[:5]\n",
    "        if any('.' in str(key) for key in sample_keys):\n",
    "            print(\"\\n✓ This appears to be a state_dict (model weights only)\")\n",
    "            print(\"Sample keys:\", sample_keys)\n",
    "            \n",
    "            # We need to instantiate the model architecture\n",
    "            # Try to infer the architecture from the state dict\n",
    "            print(\"\\n--- Inferring Model Architecture ---\")\n",
    "            \n",
    "            # Analyze the state dict to understand the network\n",
    "            layer_info = {}\n",
    "            for key, value in checkpoint.items():\n",
    "                if 'weight' in key:\n",
    "                    layer_name = key.replace('.weight', '')\n",
    "                    layer_info[layer_name] = value.shape\n",
    "                    \n",
    "            print(\"Detected layers:\")\n",
    "            for layer, shape in list(layer_info.items())[:10]:\n",
    "                print(f\"  {layer}: {shape}\")\n",
    "            \n",
    "            # Determine input and output dimensions\n",
    "            first_layer = next(iter(layer_info.items()))\n",
    "            last_layer = list(layer_info.items())[-1]\n",
    "            \n",
    "            # Common SKRL policy network structure\n",
    "            if len(layer_info) > 0:\n",
    "                # Get input dimension (first layer's input size)\n",
    "                input_dim = first_layer[1][1] if len(first_layer[1]) > 1 else first_layer[1][0]\n",
    "                # Get output dimension (last layer's output size)\n",
    "                output_dim = last_layer[1][0]\n",
    "                \n",
    "                print(f\"\\nInferred dimensions:\")\n",
    "                print(f\"  Input dim: {input_dim} (should be 25 for your obs space)\")\n",
    "                print(f\"  Output dim: {output_dim} (should be 6 for joint actions)\")\n",
    "                \n",
    "                # Create a simple MLP that matches the state dict\n",
    "                model = create_mlp_from_state_dict(checkpoint, input_dim, output_dim, device)\n",
    "                \n",
    "                if model is not None:\n",
    "                    print(\"✓ Successfully created model from state dict\")\n",
    "                else:\n",
    "                    print(\"❌ Failed to create model from state dict\")\n",
    "                    return None, device\n",
    "            else:\n",
    "                print(\"❌ Could not determine model architecture from state dict\")\n",
    "                return None, device\n",
    "                \n",
    "        elif \"agent\" in checkpoint:\n",
    "            print(\"\\nFound 'agent' in checkpoint\")\n",
    "            print(\"Agent keys:\", checkpoint[\"agent\"].keys())\n",
    "            if \"models\" in checkpoint[\"agent\"]:\n",
    "                print(\"Models available:\", checkpoint[\"agent\"][\"models\"].keys())\n",
    "                if \"policy\" in checkpoint[\"agent\"][\"models\"]:\n",
    "                    model_data = checkpoint[\"agent\"][\"models\"][\"policy\"]\n",
    "                    print(f\"Policy type: {type(model_data)}\")\n",
    "                    \n",
    "                    # Check if it's a state dict\n",
    "                    if isinstance(model_data, dict) and any('.' in str(k) for k in model_data.keys()):\n",
    "                        print(\"Policy is a state dict, creating model...\")\n",
    "                        # Analyze and create model\n",
    "                        input_dim = 25  # Your observation space\n",
    "                        output_dim = 6  # Your action space\n",
    "                        model = create_mlp_from_state_dict(model_data, input_dim, output_dim, device)\n",
    "                    else:\n",
    "                        model = model_data\n",
    "            else:\n",
    "                print(\"❌ Unexpected agent structure\")\n",
    "                return None, device\n",
    "        else:\n",
    "            print(\"❌ Unexpected checkpoint structure\")\n",
    "            return None, device\n",
    "    else:\n",
    "        print(f\"❌ Unexpected checkpoint type: {type(checkpoint)}\")\n",
    "        return None, device\n",
    "    \n",
    "    # Set to eval mode if we have a model\n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nModel loaded successfully!\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "\n",
    "def create_mlp_from_state_dict(state_dict, input_dim, output_dim, device):\n",
    "    \"\"\"Create an MLP model that matches the state dict structure\"\"\"\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    # Analyze state dict to find layer sizes\n",
    "    layers = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if 'weight' in key:\n",
    "            # Extract layer number/name\n",
    "            parts = key.split('.')\n",
    "            if len(parts) >= 2:\n",
    "                layer_idx = parts[0] if parts[0].isdigit() else parts[1] if len(parts) > 1 and parts[1].isdigit() else parts[0]\n",
    "                if 'weight' in key:\n",
    "                    layers[layer_idx] = value.shape\n",
    "    \n",
    "    # Sort layers by name/index\n",
    "    sorted_layers = sorted(layers.items(), key=lambda x: x[0])\n",
    "    \n",
    "    print(f\"\\nCreating MLP with structure:\")\n",
    "    print(f\"  Input: {input_dim}\")\n",
    "    \n",
    "    # Build the network\n",
    "    class PolicyNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList()\n",
    "            \n",
    "            prev_size = input_dim\n",
    "            for i, (name, shape) in enumerate(sorted_layers):\n",
    "                out_size, in_size = shape\n",
    "                if i == 0 and in_size != input_dim:\n",
    "                    print(f\"  ⚠️ Warning: First layer expects {in_size} inputs, but obs space is {input_dim}\")\n",
    "                \n",
    "                print(f\"  Layer {name}: {in_size} -> {out_size}\")\n",
    "                self.layers.append(nn.Linear(in_size, out_size))\n",
    "                prev_size = out_size\n",
    "            \n",
    "            # Add activation functions (assuming ReLU for hidden layers)\n",
    "            self.activation = nn.ReLU()\n",
    "            self.output_activation = nn.Tanh()  # Common for RL policies\n",
    "            \n",
    "        def forward(self, x):\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                x = layer(x)\n",
    "                if i < len(self.layers) - 1:\n",
    "                    x = self.activation(x)\n",
    "                else:\n",
    "                    # Last layer - might use tanh for bounded actions\n",
    "                    x = self.output_activation(x)\n",
    "            return x\n",
    "    \n",
    "    try:\n",
    "        model = PolicyNetwork()\n",
    "        \n",
    "        # Load the state dict\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Test forward pass\n",
    "        test_input = torch.randn(1, input_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            test_output = model(test_input)\n",
    "            print(f\"  Output: {output_dim} (shape: {test_output.shape})\")\n",
    "            \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error creating model: {e}\")\n",
    "        \n",
    "        # Try a simpler approach - just create a standard MLP\n",
    "        print(\"\\nTrying standard MLP architecture...\")\n",
    "        class StandardMLP(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, 256)\n",
    "                self.fc2 = nn.Linear(256, 256)\n",
    "                self.fc3 = nn.Linear(256, output_dim)\n",
    "                self.activation = nn.ReLU()\n",
    "                self.output_activation = nn.Tanh()\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = self.activation(self.fc1(x))\n",
    "                x = self.activation(self.fc2(x))\n",
    "                x = self.output_activation(self.fc3(x))\n",
    "                return x\n",
    "        \n",
    "        try:\n",
    "            model = StandardMLP()\n",
    "            # Try to load whatever weights match\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict, strict=False)\n",
    "            model.to(device)\n",
    "            print(\"✓ Created standard MLP and loaded matching weights\")\n",
    "            return model\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Failed with standard MLP: {e2}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def create_mock_observations(batch_size: int = 5):\n",
    "    \"\"\"Create mock observations matching your IsaacLab config\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Creating Mock Observations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Based on your ObservationsCfg:\n",
    "    # - joint_pos (6): relative joint positions\n",
    "    # - joint_vel (6): relative joint velocities  \n",
    "    # - pose_command (7): target position (3) + quaternion (4)\n",
    "    # - actions (6): last action\n",
    "    # Total: 25 dimensions\n",
    "    \n",
    "    observations = []\n",
    "    descriptions = []\n",
    "    \n",
    "    # Default joint positions (from your IsaacLab config)\n",
    "    default_joints = np.array([0.0, -1.712, 1.712, 0.0, 0.0, 0.0])\n",
    "    \n",
    "    # Test case 1: At default position, no velocity, target straight ahead\n",
    "    obs1 = np.zeros(25)\n",
    "    obs1[0:6] = 0.0  # At default position (relative = 0)\n",
    "    obs1[6:12] = 0.0  # No velocity\n",
    "    obs1[12:15] = [0.3, 0.0, 0.3]  # Target position\n",
    "    obs1[15:19] = [1.0, 0.0, 0.0, 0.0]  # Target orientation (quaternion w,x,y,z)\n",
    "    obs1[19:25] = 0.0  # No previous action\n",
    "    observations.append(obs1)\n",
    "    descriptions.append(\"At home position, target ahead\")\n",
    "    \n",
    "    # Test case 2: Slightly off default, target to the right\n",
    "    obs2 = np.zeros(25)\n",
    "    obs2[0:6] = [0.1, -0.1, 0.1, 0.0, 0.0, 0.0]  # Slightly off default\n",
    "    obs2[6:12] = [0.05, -0.05, 0.0, 0.0, 0.0, 0.0]  # Small velocities\n",
    "    obs2[12:15] = [0.2, 0.2, 0.3]  # Target to the right\n",
    "    obs2[15:19] = [1.0, 0.0, 0.0, 0.0]\n",
    "    obs2[19:25] = [0.01, -0.01, 0.01, 0.0, 0.0, 0.0]  # Small previous action\n",
    "    observations.append(obs2)\n",
    "    descriptions.append(\"Slightly off home, target right\")\n",
    "    \n",
    "    # Test case 3: Random position within reasonable bounds\n",
    "    obs3 = np.zeros(25)\n",
    "    obs3[0:6] = np.random.uniform(-0.5, 0.5, 6)  # Random relative positions\n",
    "    obs3[6:12] = np.random.uniform(-0.1, 0.1, 6)  # Random velocities\n",
    "    obs3[12:15] = [0.25, -0.1, 0.35]  # Target to the left\n",
    "    obs3[15:19] = [1.0, 0.0, 0.0, 0.0]\n",
    "    obs3[19:25] = np.random.uniform(-0.1, 0.1, 6)\n",
    "    observations.append(obs3)\n",
    "    descriptions.append(\"Random position, target left\")\n",
    "    \n",
    "    # Test case 4: Near target position\n",
    "    obs4 = np.zeros(25)\n",
    "    obs4[0:6] = [0.2, -0.3, 0.3, 0.1, 0.0, 0.0]  # Position near a reaching pose\n",
    "    obs4[6:12] = [0.02, -0.02, 0.01, 0.0, 0.0, 0.0]  # Small velocities\n",
    "    obs4[12:15] = [0.3, 0.0, 0.3]  # Target position (close to current)\n",
    "    obs4[15:19] = [1.0, 0.0, 0.0, 0.0]\n",
    "    obs4[19:25] = [0.02, -0.02, 0.01, 0.0, 0.0, 0.0]\n",
    "    observations.append(obs4)\n",
    "    descriptions.append(\"Near target position\")\n",
    "    \n",
    "    # Test case 5: Moving fast towards target\n",
    "    obs5 = np.zeros(25)\n",
    "    obs5[0:6] = [0.15, -0.2, 0.2, 0.05, 0.0, 0.0]\n",
    "    obs5[6:12] = [0.2, -0.15, 0.1, 0.05, 0.0, 0.0]  # Higher velocities\n",
    "    obs5[12:15] = [0.35, 0.1, 0.4]  # Target further away\n",
    "    obs5[15:19] = [1.0, 0.0, 0.0, 0.0]\n",
    "    obs5[19:25] = [0.1, -0.08, 0.05, 0.02, 0.0, 0.0]  # Larger previous action\n",
    "    observations.append(obs5)\n",
    "    descriptions.append(\"Moving fast towards distant target\")\n",
    "    \n",
    "    return np.array(observations), descriptions\n",
    "\n",
    "\n",
    "def test_model_inference(model, device, observations, descriptions):\n",
    "    \"\"\"Test model inference with mock observations\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: Testing Model Inference\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()\n",
    "    actions_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (obs, desc) in enumerate(zip(observations, descriptions)):\n",
    "            print(f\"\\n--- Test Case {i+1}: {desc} ---\")\n",
    "            \n",
    "            # Print observation summary\n",
    "            print(f\"Observation shape: {obs.shape}\")\n",
    "            print(f\"  Joint pos (rel): {obs[0:6].round(3)}\")\n",
    "            print(f\"  Joint vel (rel): {obs[6:12].round(3)}\")\n",
    "            print(f\"  Target pos: {obs[12:15].round(3)}\")\n",
    "            print(f\"  Target quat: {obs[15:19].round(3)}\")\n",
    "            print(f\"  Last action: {obs[19:25].round(3)}\")\n",
    "            \n",
    "            # Convert to tensor and add batch dimension\n",
    "            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get action from model\n",
    "            try:\n",
    "                action_tensor = model(obs_tensor)\n",
    "                action = action_tensor.cpu().numpy().squeeze()\n",
    "                actions_list.append(action)\n",
    "                \n",
    "                print(f\"\\nAction output: {action.round(3)}\")\n",
    "                print(f\"  Shape: {action.shape}\")\n",
    "                print(f\"  Mean: {action.mean():.3f}\")\n",
    "                print(f\"  Std: {action.std():.3f}\")\n",
    "                print(f\"  Min: {action.min():.3f}\")\n",
    "                print(f\"  Max: {action.max():.3f}\")\n",
    "                print(f\"  Range: [{action.min():.3f}, {action.max():.3f}]\")\n",
    "                \n",
    "                # Check if actions are reasonable\n",
    "                if np.abs(action).max() > 10:\n",
    "                    print(\"  ⚠️ WARNING: Large action values detected!\")\n",
    "                elif np.abs(action).max() < 0.001:\n",
    "                    print(\"  ⚠️ WARNING: Very small action values detected!\")\n",
    "                else:\n",
    "                    print(\"  ✓ Action values seem reasonable\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error during inference: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    return np.array(actions_list) if actions_list else None\n",
    "\n",
    "\n",
    "def analyze_actions(actions, observations, descriptions):\n",
    "    \"\"\"Analyze the pattern of actions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: Analyzing Action Patterns\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if actions is None or len(actions) == 0:\n",
    "        print(\"No actions to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\n--- Overall Action Statistics ---\")\n",
    "    print(f\"Mean across all actions: {actions.mean():.3f}\")\n",
    "    print(f\"Std across all actions: {actions.std():.3f}\")\n",
    "    print(f\"Min value: {actions.min():.3f}\")\n",
    "    print(f\"Max value: {actions.max():.3f}\")\n",
    "    print(f\"Absolute max: {np.abs(actions).max():.3f}\")\n",
    "    \n",
    "    # Per-joint statistics\n",
    "    print(\"\\n--- Per-Joint Statistics ---\")\n",
    "    for j in range(actions.shape[1]):\n",
    "        print(f\"Joint {j+1}: mean={actions[:, j].mean():.3f}, \"\n",
    "              f\"std={actions[:, j].std():.3f}, \"\n",
    "              f\"range=[{actions[:, j].min():.3f}, {actions[:, j].max():.3f}]\")\n",
    "    \n",
    "    # Check for patterns\n",
    "    print(\"\\n--- Pattern Analysis ---\")\n",
    "    \n",
    "    # Check if actions correlate with distance to target\n",
    "    for i, (action, obs, desc) in enumerate(zip(actions, observations, descriptions)):\n",
    "        target_pos = obs[12:15]\n",
    "        # Simple distance metric (this is approximate)\n",
    "        distance_to_target = np.linalg.norm(target_pos - np.array([0.2, 0.0, 0.2]))\n",
    "        action_magnitude = np.linalg.norm(action)\n",
    "        print(f\"Case {i+1}: action_magnitude={action_magnitude:.3f}, \"\n",
    "              f\"approx_distance={distance_to_target:.3f}\")\n",
    "    \n",
    "    # Visualize if matplotlib is available\n",
    "    try:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "        fig.suptitle('Action Analysis', fontsize=16)\n",
    "        \n",
    "        # Plot action values for each test case\n",
    "        for i in range(min(6, actions.shape[1])):\n",
    "            ax = axes[i // 3, i % 3]\n",
    "            ax.bar(range(len(actions)), actions[:, i])\n",
    "            ax.set_title(f'Joint {i+1} Actions')\n",
    "            ax.set_xlabel('Test Case')\n",
    "            ax.set_ylabel('Action Value')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('action_analysis.png')\n",
    "        print(\"\\n✓ Saved action analysis plot to 'action_analysis.png'\")\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"\\n(Matplotlib not available for visualization)\")\n",
    "\n",
    "\n",
    "def suggest_deployment_parameters(actions):\n",
    "    \"\"\"Suggest deployment parameters based on action analysis\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: Deployment Recommendations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if actions is None or len(actions) == 0:\n",
    "        print(\"No actions available for recommendations\")\n",
    "        return\n",
    "    \n",
    "    max_action = np.abs(actions).max()\n",
    "    mean_action = np.abs(actions).mean()\n",
    "    \n",
    "    print(\"\\nBased on the model outputs:\")\n",
    "    \n",
    "    # Suggest action scaling\n",
    "    if max_action > 5:\n",
    "        scale = 0.005\n",
    "        print(f\"⚠️ Actions are large (max={max_action:.2f})\")\n",
    "        print(f\"   Recommended position_scale: {scale}\")\n",
    "        print(f\"   This will limit max position change to ~{max_action * scale:.3f} rad\")\n",
    "    elif max_action > 1:\n",
    "        scale = 0.01\n",
    "        print(f\"⚠️ Actions are moderate (max={max_action:.2f})\")\n",
    "        print(f\"   Recommended position_scale: {scale}\")\n",
    "        print(f\"   This will limit max position change to ~{max_action * scale:.3f} rad\")\n",
    "    elif max_action < 0.01:\n",
    "        scale = 1.0\n",
    "        print(f\"⚠️ Actions are very small (max={max_action:.4f})\")\n",
    "        print(f\"   Recommended position_scale: {scale}\")\n",
    "        print(f\"   Consider checking if model is outputting correctly\")\n",
    "    else:\n",
    "        scale = 0.1\n",
    "        print(f\"✓ Actions are in reasonable range (max={max_action:.2f})\")\n",
    "        print(f\"   Recommended position_scale: {scale}\")\n",
    "        print(f\"   This will limit max position change to ~{max_action * scale:.3f} rad\")\n",
    "    \n",
    "    # Suggest safety limits\n",
    "    print(f\"\\nRecommended safety limits:\")\n",
    "    print(f\"  max_joint_velocity: 0.2 rad/s (start conservative)\")\n",
    "    print(f\"  max_joint_acceleration: 0.3 rad/s²\")\n",
    "    print(f\"  control_frequency: 10 Hz\")\n",
    "    \n",
    "    # Suggest testing approach\n",
    "    print(f\"\\nTesting approach:\")\n",
    "    print(f\"  1. Run in debug mode first to verify observations\")\n",
    "    print(f\"  2. Start with position_scale = {scale * 0.1:.4f} (10% of recommended)\")\n",
    "    print(f\"  3. Gradually increase to {scale:.4f}\")\n",
    "    print(f\"  4. Monitor actual vs commanded positions\")\n",
    "    \n",
    "    # Check for potential issues\n",
    "    print(f\"\\nPotential issues to watch for:\")\n",
    "    if np.std(actions) < 0.001:\n",
    "        print(f\"  ⚠️ Very low action variance - model might be outputting constants\")\n",
    "    if np.any(np.isnan(actions)) or np.any(np.isinf(actions)):\n",
    "        print(f\"  ❌ NaN or Inf values detected in actions!\")\n",
    "    \n",
    "    # Action type inference\n",
    "    print(f\"\\nInferred action type based on range:\")\n",
    "    if max_action < 0.1:\n",
    "        print(f\"  Likely: Direct joint position targets (radians)\")\n",
    "    elif max_action < 2:\n",
    "        print(f\"  Likely: Normalized actions [-1, 1] for position/velocity control\")\n",
    "    else:\n",
    "        print(f\"  Likely: Unnormalized outputs (may need scaling/clipping)\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your model\n",
    "    MODEL_PATH = \"/home/urkui-3/Documents/Ales/IsaacLab/runs/torch/Isaac-Reach-UR3e-v0/UR3e/checkpoints/best_agent.pt\"\n",
    "    \n",
    "    print(\"UR3e Model Testing Suite\")\n",
    "    print(\"========================\\n\")\n",
    "    \n",
    "    # Step 1: Load model\n",
    "    model, device = test_model_loading(MODEL_PATH)\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"\\n❌ Failed to load model. Please check the checkpoint structure.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Step 2: Create mock observations\n",
    "    observations, descriptions = create_mock_observations()\n",
    "    \n",
    "    # Step 3: Test inference\n",
    "    actions = test_model_inference(model, device, observations, descriptions)\n",
    "    \n",
    "    # Step 4: Analyze actions\n",
    "    if actions is not None:\n",
    "        analyze_actions(actions, observations, descriptions)\n",
    "        \n",
    "        # Step 5: Get deployment recommendations\n",
    "        suggest_deployment_parameters(actions)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Review the action outputs above\")\n",
    "    print(\"2. Adjust the deployment script based on recommendations\")\n",
    "    print(\"3. Test with debug mode on the real robot\")\n",
    "    print(\"4. Gradually enable real control with safety limits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
